{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Copy of week14_quantization_practice_additional.ipynb","provenance":[{"file_id":"https://github.com/ml-mipt/ml-mipt/blob/advanced/week14_segmentation_model_compression/week14_quantization_practice_additional.ipynb","timestamp":1598825790640}]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0a4b9631154640c096eba418667b8739":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6ff3f2092f974e36bcfc0e314b95e874","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9a9635936ec6432484594c2ffc4af3f7","IPY_MODEL_72574731926540a8a1ccf1547362155c"]}},"6ff3f2092f974e36bcfc0e314b95e874":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9a9635936ec6432484594c2ffc4af3f7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_141c6ec2c79049248d15a69cf40c5bc2","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1fbd1775ef6c4f98ab341541af2bec54"}},"72574731926540a8a1ccf1547362155c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9b6e5d23e3864c2285c53d8349f52dd5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9920512/? [00:19&lt;00:00, 1075673.02it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b6e4b268694a4a26a7134428b30066af"}},"141c6ec2c79049248d15a69cf40c5bc2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1fbd1775ef6c4f98ab341541af2bec54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9b6e5d23e3864c2285c53d8349f52dd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b6e4b268694a4a26a7134428b30066af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"44aad975bce54608bd68309fe18de899":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_271f2e533ca74c778c25a7bfeb9532b3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cef6c027ba8f4384a21a02313bf13fd0","IPY_MODEL_fa4cd3035561404ebfa2531b9e8b52e1"]}},"271f2e533ca74c778c25a7bfeb9532b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cef6c027ba8f4384a21a02313bf13fd0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2aec90b291d34c6fb975a1a4e0562b72","_dom_classes":[],"description":"  0%","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_80f7ef1b7fb24cd8abf0996c26dde835"}},"fa4cd3035561404ebfa2531b9e8b52e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_74f717233bbc4fa6af13820ffe468c7d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/28881 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1d60dd063bbb4debb6dea13984ec8606"}},"2aec90b291d34c6fb975a1a4e0562b72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"80f7ef1b7fb24cd8abf0996c26dde835":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"74f717233bbc4fa6af13820ffe468c7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1d60dd063bbb4debb6dea13984ec8606":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1a584a747a9e4b9aa23d478aba28e51f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f0eda9a342124c34a44e63eac6a94ca9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_80d57ea85d464ad8896e052130cc767c","IPY_MODEL_f8183b916a6c47dc8519f799b301860d"]}},"f0eda9a342124c34a44e63eac6a94ca9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"80d57ea85d464ad8896e052130cc767c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e4909bdc1dd541148af348b7d27f1091","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5ed02758e8f74034ac204979f470eb81"}},"f8183b916a6c47dc8519f799b301860d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6c61c3beac2349a48fdfcf7c7495f0ad","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1654784/? [00:02&lt;00:00, 659325.60it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2bfe417af002468baf7e00228cc3d60f"}},"e4909bdc1dd541148af348b7d27f1091":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5ed02758e8f74034ac204979f470eb81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c61c3beac2349a48fdfcf7c7495f0ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2bfe417af002468baf7e00228cc3d60f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"926124fde07f49268f0a247cd74d4168":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_be15e091357847549fdbda67fcd2eaea","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6c2639753703444cb2f81cda14bd0640","IPY_MODEL_b6876f4ce3b147a883331582aaef473f"]}},"be15e091357847549fdbda67fcd2eaea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c2639753703444cb2f81cda14bd0640":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_953bf168ffb741a7920942ccc90f9ef9","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aa86fafb688841b2b4b7e1d4a0040c40"}},"b6876f4ce3b147a883331582aaef473f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_17804f009d364ca19916750748c525c1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 8192/? [00:01&lt;00:00, 4555.57it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_789efa5e6bcc4e30ad1810a9b33d5661"}},"953bf168ffb741a7920942ccc90f9ef9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"aa86fafb688841b2b4b7e1d4a0040c40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"17804f009d364ca19916750748c525c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"789efa5e6bcc4e30ad1810a9b33d5661":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"nt9TRHfVakL2","colab_type":"text"},"source":["### CNN Quantization Practice\n","\n","This tutorial implements [DoReFa-Net](https://arxiv.org/abs/1606.06160) weight and activation quantization scheme. Gradient quantization is not implemented, however is described in the original paper.\n","\n","Code inspiration is taken from randomly found [github](https://github.com/zzzxxxttt/pytorch_DoReFaNet) due to the fact that current notebook author has a lack of creativity."]},{"cell_type":"code","metadata":{"id":"XYqjiL9bakL3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598825396132,"user_tz":-180,"elapsed":4405,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","import numpy as np\n","from tqdm.auto import tqdm\n","from matplotlib import pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"CT-qt2IcakL6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598825396133,"user_tz":-180,"elapsed":4395,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["use_cuda = True\n","torch.manual_seed(42)\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","batch_size = 256\n","test_batch_size = 128\n","epochs = 10\n","lr = 1e-3\n","weight_decay = 0"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZU_VgtedakL9","colab_type":"text"},"source":["Let's use MNIST dataset and evaluate one of the current state-of-the-art methods of CNN quantization.\n","\n","We will implement a simple CNN model yet capable of reaching high performance on MNIST.\n","\n","Then we will implement a quantized CNN capable of reaching same performance on it."]},{"cell_type":"markdown","metadata":{"id":"bdAGzP5nakL-","colab_type":"text"},"source":["#### Data loading\n","\n","We are developing quantized neural networks, so we need to feed images into CNN in different manner:\n","pixel absolute values are no longer needed to be less than 1."]},{"cell_type":"code","metadata":{"id":"-1_gJ-thakL-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":369,"referenced_widgets":["0a4b9631154640c096eba418667b8739","6ff3f2092f974e36bcfc0e314b95e874","9a9635936ec6432484594c2ffc4af3f7","72574731926540a8a1ccf1547362155c","141c6ec2c79049248d15a69cf40c5bc2","1fbd1775ef6c4f98ab341541af2bec54","9b6e5d23e3864c2285c53d8349f52dd5","b6e4b268694a4a26a7134428b30066af","44aad975bce54608bd68309fe18de899","271f2e533ca74c778c25a7bfeb9532b3","cef6c027ba8f4384a21a02313bf13fd0","fa4cd3035561404ebfa2531b9e8b52e1","2aec90b291d34c6fb975a1a4e0562b72","80f7ef1b7fb24cd8abf0996c26dde835","74f717233bbc4fa6af13820ffe468c7d","1d60dd063bbb4debb6dea13984ec8606","1a584a747a9e4b9aa23d478aba28e51f","f0eda9a342124c34a44e63eac6a94ca9","80d57ea85d464ad8896e052130cc767c","f8183b916a6c47dc8519f799b301860d","e4909bdc1dd541148af348b7d27f1091","5ed02758e8f74034ac204979f470eb81","6c61c3beac2349a48fdfcf7c7495f0ad","2bfe417af002468baf7e00228cc3d60f","926124fde07f49268f0a247cd74d4168","be15e091357847549fdbda67fcd2eaea","6c2639753703444cb2f81cda14bd0640","b6876f4ce3b147a883331582aaef473f","953bf168ffb741a7920942ccc90f9ef9","aa86fafb688841b2b4b7e1d4a0040c40","17804f009d364ca19916750748c525c1","789efa5e6bcc4e30ad1810a9b33d5661"]},"executionInfo":{"status":"ok","timestamp":1598825398678,"user_tz":-180,"elapsed":6055,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"a9500807-dcf7-472a-a0eb-317cf381b70c"},"source":["def to_signed_char(tensor):\n","    # note that we convert image values from float32 range([-0.5, 0.5]) to signed char range [-128,127]\n","    return torch.floor(255.0 * tensor)\n","\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data', train=True, download=True,\n","                   transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.5,), (1, )),\n","                       \n","                   ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (1, )),\n","                   ])),\n","    batch_size=test_batch_size, shuffle=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a4b9631154640c096eba418667b8739","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44aad975bce54608bd68309fe18de899","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a584a747a9e4b9aa23d478aba28e51f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"926124fde07f49268f0a247cd74d4168","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","Processing...\n","Done!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"E3ACK0odakMA","colab_type":"text"},"source":["\n","#### Training and Validation Functions"]},{"cell_type":"code","metadata":{"id":"P4MV_CgiakMB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1598825400191,"user_tz":-180,"elapsed":646,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"5c8dde4c-c36c-42ca-80f8-0b055bc1ff1b"},"source":["def train(model, device, train_loader, optimizer, epoch, **kwargs):\n","    log_interval = kwargs.get('log_interval', 10)\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(to_signed_char(data))\n","        loss = F.nll_loss(output, target)\n","        loss.backward(retain_graph=True)\n","        optimizer.step()\n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","\n","\n","def test(model, device, test_loader, **kwargs):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(to_signed_char(data))\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zfIkmFKmakME","colab_type":"text"},"source":["#### CNN Model\n","\n","Let's implement something really easy, __conv->bn->relu->pool__ will be enough."]},{"cell_type":"code","metadata":{"id":"VVJkHwBaakME","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598825418280,"user_tz":-180,"elapsed":622,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(1, 32, 3, 1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU()\n","        )\n","        self.maxpool_2x2 = nn.MaxPool2d((2,2))\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(32, 64, 3, 1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU()\n","        )\n","        self.fc1 = nn.Linear(64, 10)\n","        self.gmp = nn.AdaptiveMaxPool2d(1)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.maxpool_2x2(x)\n","        x = self.conv2(x)\n","        x = self.gmp(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        output = F.log_softmax(x, dim=1)\n","        return output"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWpvYA32akMH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598825546140,"user_tz":-180,"elapsed":118857,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"cf5a386a-9a99-4242-f539-42c2b9c041ac"},"source":["model = Net().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n","for epoch in range(1, epochs + 1):\n","    train(model, device, train_loader, optimizer, epoch, log_interval=50)\n","    test(model, device, test_loader)\n","    scheduler.step()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.221831\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.167750\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.607980\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.418338\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.299370\n","\n","Test set: Average loss: 0.2501, Accuracy: 9488/10000 (95%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.253335\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.244047\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.180606\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.176134\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.175280\n","\n","Test set: Average loss: 0.1530, Accuracy: 9639/10000 (96%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.154688\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.165719\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.160883\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.113537\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.134501\n","\n","Test set: Average loss: 0.1257, Accuracy: 9680/10000 (97%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.121172\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.148401\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.121280\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.138218\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.096037\n","\n","Test set: Average loss: 0.1010, Accuracy: 9739/10000 (97%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.110671\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.093714\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.069992\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.045154\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.063530\n","\n","Test set: Average loss: 0.0931, Accuracy: 9726/10000 (97%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.079940\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.123278\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.067925\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.056664\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.096871\n","\n","Test set: Average loss: 0.0881, Accuracy: 9752/10000 (98%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.109479\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.056837\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.059573\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.057877\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.098948\n","\n","Test set: Average loss: 0.0824, Accuracy: 9752/10000 (98%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.035671\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.062758\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.076629\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.051662\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.052600\n","\n","Test set: Average loss: 0.0756, Accuracy: 9790/10000 (98%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.065809\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.042710\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.073251\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.080297\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.059565\n","\n","Test set: Average loss: 0.0727, Accuracy: 9788/10000 (98%)\n","\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.037609\n","Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.039748\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.034646\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.061320\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.032137\n","\n","Test set: Average loss: 0.0728, Accuracy: 9775/10000 (98%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"usLLRWaJakMK","colab_type":"text"},"source":["#### Quantization Layers\n","\n","Not let's implement basic building blocks for DoReFa-Net quantization scheme.\n","Firstly, we will need to implement uniform quantizer.\n","\n","Given real value $r\\in [0;1]$ uniform quantized value $r_0$ may be calculated:\n","\n","$r_0 = \\frac{1}{2^k-1} round((2^k-1) * r)$\n","\n","The calculated value may be saved as floating point number, or converted to fixed point representation by multiplying it on $2^{\\text{n_bits}}$"]},{"cell_type":"code","metadata":{"id":"VU-_qdtWakMK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598825559808,"user_tz":-180,"elapsed":686,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["def get_uniform_quantizer(n_bits):\n","    class qfn(torch.autograd.Function):\n","        @staticmethod\n","        def forward(ctx, in_tensor):\n","            if n_bits == 32:\n","                # do not apply quantizer for full-bitwidth values\n","                out_tensor = in_tensor\n","            elif n_bits == 1:\n","                # Binary Weight Net implementation\n","                out_tensor = torch.sign(in_tensor)\n","            else:\n","                n = float(2 ** n_bits - 1)\n","                out = torch.round(in_tensor * n) / n\n","            return out\n","\n","        @staticmethod\n","        def backward(ctx, grad_output):\n","            grad_input = grad_output.clone()\n","            return grad_input\n","    return qfn().apply"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5KyPBdy9akMN","colab_type":"text"},"source":["Then we will need separate quantization functions for weights and activations.\n","For weights the following formula is implemented:\n","$$\n","r_0 = 2 \\cdot\\textit{quantize_k}(\\frac{tanh(r)}{2 * max|tanh(r)|} + \\frac{1}{2}) - 1\n","$$\n","For activations uniform quantization is applied, as defined above."]},{"cell_type":"code","metadata":{"id":"7vKpZhMwakMN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598825560260,"user_tz":-180,"elapsed":483,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["class weight_quantize_fn(nn.Module):\n","    def __init__(self, n_bits):\n","        super(weight_quantize_fn, self).__init__()\n","        assert n_bits <= 8 or n_bits == 32\n","        self.n_bits = n_bits\n","        self.value_range = 2 ** self.n_bits - 1\n","        self.uniform_q = get_uniform_quantizer(n_bits)\n","\n","    def forward(self, x):\n","        if self.n_bits == 32:\n","            weight_q = x\n","        elif self.n_bits == 1:\n","            E = x.abs().mean().detach()\n","            weight_q = self.uniform_q(x / E) * E\n","        else:\n","            weight = x.tanh()\n","            weight = weight / 2 / weight.abs().max() + 0.5\n","            # weight_q = 2 * self.uniform_q(weight) - 1 # for original implementation with real values\n","            weight_q = self.uniform_q(weight)\n","            weight_q = self.value_range * weight_q - (self.value_range + 1) / 2\n","        return weight_q\n","\n","\n","class activation_quantize_fn(nn.Module):\n","    def __init__(self, n_bits):\n","        super(activation_quantize_fn, self).__init__()\n","        assert n_bits <= 8 or n_bits == 32\n","        self.n_bits = n_bits\n","        self.uniform_q = get_uniform_quantizer(n_bits)\n","    \n","    def forward(self, x):\n","        if self.n_bits == 32:\n","            activation_q = x\n","        else:\n","            activation_q = (2 ** self.n_bits - 1) * self.uniform_q(torch.clamp(x, 0, 1))\n","        return activation_q"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"apobpdR6akMP","colab_type":"text"},"source":["And now our building blocks allow us to implement basic convolution and activation functions."]},{"cell_type":"code","metadata":{"id":"NXp8niqKakMP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598825562039,"user_tz":-180,"elapsed":615,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["class DRFConv2d(nn.Conv2d):\n","    def __init__(self, n_bits, in_channels, out_channels, kernel_size, stride=1,\n","                 padding=0, dilation=1, groups=1, bias=False):\n","        super(DRFConv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n","                                 padding, dilation, groups, bias)\n","        self.n_bits = n_bits\n","        self.quantize_fn = weight_quantize_fn(n_bits)\n","\n","    def forward(self, in_tensor, order=None):\n","        weight_q = self.quantize_fn(self.weight)\n","        return F.conv2d(in_tensor, weight_q, self.bias, self.stride,\n","                  self.padding, self.dilation, self.groups)\n","\n","class DRFActivation(nn.Module):\n","    def __init__(self, n_bits):\n","        super(DRFActivation, self).__init__()\n","        self.n_bits = n_bits\n","        self.quantize_fn = activation_quantize_fn(n_bits)\n","\n","    def forward(self, in_tensor):\n","        if self.n_bits >= 32:\n","            return in_tensor\n","        out_tensor = self.quantize_fn(in_tensor)\n","        return out_tensor"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DK3ZI10uakMS","colab_type":"text"},"source":["Let's implement the quantized CNN the same as the full-precision CNN"]},{"cell_type":"code","metadata":{"id":"XrGCl2N_akMS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598825601519,"user_tz":-180,"elapsed":642,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["class QNet(nn.Module):\n","    def __init__(self, n_bits=8):\n","        super(QNet, self).__init__()\n","        self.n_bits = n_bits\n","        self.conv1 = nn.Sequential(\n","            DRFConv2d(self.n_bits, 1, 32, 3, 1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            DRFActivation(self.n_bits)\n","        )\n","        self.maxpool_2x2 = nn.MaxPool2d((2,2))\n","        self.conv2 = nn.Sequential(\n","            DRFConv2d(self.n_bits, 32, 64, 3, 1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            DRFActivation(self.n_bits)\n","        )\n","        self.fc1 = DRFConv2d(self.n_bits, 64, 10, 1)\n","        self.gmp = nn.AdaptiveMaxPool2d(1)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.maxpool_2x2(x)\n","        x = self.conv2(x)\n","        x = self.gmp(x)\n","        x = self.fc1(x)\n","        x = torch.flatten(x, 1)\n","        output = x / 100000\n","        output = F.log_softmax(output, dim=1)\n","        return output"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBLkVMTaakMU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1598825636931,"user_tz":-180,"elapsed":607,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"710c3b92-0343-4be1-855e-5c7553094329"},"source":["# sanity check\n","qmodel = QNet().to(torch.device('cpu'))\n","in_tensor = torch.floor(torch.from_numpy(np.random.randint(0,255,size=(28,28))).view(1, 1, 28, 28).type(torch.float)) - 127\n","res = qmodel(in_tensor)\n","print(res.detach().numpy()[0])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[-4.336321  -3.9920714 -2.5487714 -2.2529712 -3.3622212 -1.0544713\n"," -2.3345714 -1.9036212 -2.6635213 -2.4467711]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"eyYbhdpuakMW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598825754638,"user_tz":-180,"elapsed":115914,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"830d63db-4400-4dba-f1cf-2638fc1bc87d"},"source":["model = QNet().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n","for epoch in range(1, epochs + 1):\n","    train(model, device, train_loader, optimizer, epoch, log_interval=100)\n","    test(model, device, test_loader)\n","    scheduler.step()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.950861\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.221817\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.925753\n","\n","Test set: Average loss: 1.9566, Accuracy: 2992/10000 (30%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.949815\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.747316\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.541326\n","\n","Test set: Average loss: 1.4624, Accuracy: 6821/10000 (68%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.403982\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.329035\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.224247\n","\n","Test set: Average loss: 1.1424, Accuracy: 8488/10000 (85%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.139566\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.013708\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.027334\n","\n","Test set: Average loss: 0.9652, Accuracy: 8836/10000 (88%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.932221\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.893078\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.816760\n","\n","Test set: Average loss: 0.8170, Accuracy: 9144/10000 (91%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.806605\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.741615\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.768409\n","\n","Test set: Average loss: 0.7316, Accuracy: 9212/10000 (92%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.701766\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.619817\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.668022\n","\n","Test set: Average loss: 0.6558, Accuracy: 9334/10000 (93%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.631922\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.671627\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.562795\n","\n","Test set: Average loss: 0.5985, Accuracy: 9374/10000 (94%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.575598\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.563121\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.545583\n","\n","Test set: Average loss: 0.5475, Accuracy: 9414/10000 (94%)\n","\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.534626\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.492063\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.474636\n","\n","Test set: Average loss: 0.5036, Accuracy: 9415/10000 (94%)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ckDZoB9nakMZ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}