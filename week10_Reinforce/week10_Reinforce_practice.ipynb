{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Py3 research env","language":"python","name":"py3_research"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"Copy of week10_Reinforce_practice.ipynb","provenance":[{"file_id":"https://github.com/ml-mipt/ml-mipt/blob/advanced/week10_Reinforce/week10_Reinforce_practice.ipynb","timestamp":1591575283270}],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Vb4DgDG6BzKZ","colab_type":"text"},"source":["## week10\n","### REINFORCE in PyTorch\n","__This notebook is based on [Practical_RL week06](https://github.com/yandexdataschool/Practical_RL/tree/master/week06_policy_based) materials__\n","\n","Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","metadata":{"id":"9EADgcZ2BzKa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"07b83f18-8d62-47e6-d390-af312aba9b19","executionInfo":{"status":"ok","timestamp":1591571218139,"user_tz":-180,"elapsed":22733,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# # in google colab uncomment this\n","\n","import os\n","\n","os.system('apt-get install -y xvfb')\n","os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n","os.system('apt-get install -y python-opengl ffmpeg')\n","os.system('pip install pyglet==1.5.0')\n","\n","os.system('python -m pip install -U pygame --user')\n","\n","print('setup complete')\n","\n","# XVFB will be launched if you run on a server\n","import os\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    %env DISPLAY = : 1"],"execution_count":1,"outputs":[{"output_type":"stream","text":["setup complete\n","Starting virtual X frame buffer: Xvfb.\n","env: DISPLAY=: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UnlHDuKEBzKe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":286},"outputId":"536d5ba2-0a36-4a75-edd4-f2a717c889f9","executionInfo":{"status":"ok","timestamp":1591571760540,"user_tz":-180,"elapsed":1593,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["import gym\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","env = gym.make(\"CartPole-v0\").env\n","example_state = env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","plt.imshow(env.render(\"rgb_array\"))"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7ff5fed3c668>"]},"metadata":{"tags":[]},"execution_count":6},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAASuElEQVR4nO3dfcyd9X3f8fcHY56SLDzdcTzbxLTxFtFqmOie4yj5g5KlIWgqVMoQdCJWhOSiESmRoi7QSWsiDalV1rBF68hcQUOaNIQ2D1iIjVKCFkUqDyYxxIYAJnFqezY24blpXGy+++P+mZwam/vcT9z+3ef9ko7OdX2v33XO96ccPrn883V8UlVIkvpx3Hw3IEmaGoNbkjpjcEtSZwxuSeqMwS1JnTG4JakzcxbcSS5M8liSbUmumav3kaRRk7m4jzvJIuBx4IPATuAB4PKqemTW30ySRsxcXXGvAbZV1Y+r6h+BW4CL5+i9JGmkHD9Hr7sM2DGwvxN4z9EGn3nmmbVy5co5akWS+rN9+3aefvrpHOnYXAX3pJKsB9YDnHXWWWzatGm+WpGkY874+PhRj83VUskuYMXA/vJWe1VVbaiq8aoaHxsbm6M2JGnhmavgfgBYleTsJCcAlwEb5+i9JGmkzMlSSVUdSPJx4E5gEXBTVW2di/eSpFEzZ2vcVXUHcMdcvb4kjSq/OSlJnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMz+umyJNuBF4GDwIGqGk9yOvB1YCWwHbi0qp6dWZuSpENm44r7N6pqdVWNt/1rgLurahVwd9uXJM2SuVgquRi4uW3fDFwyB+8hSSNrpsFdwF8neTDJ+lZbUlW72/YeYMkM30OSNGBGa9zA+6tqV5K3AXcl+dHgwaqqJHWkE1vQrwc466yzZtiGJI2OGV1xV9Wu9rwX+BawBngqyVKA9rz3KOduqKrxqhofGxubSRuSNFKmHdxJ3pTkLYe2gd8EtgAbgXVt2Drgtpk2KUn6pZkslSwBvpXk0Ov8RVX9nyQPALcmuRL4KXDpzNuUJB0y7eCuqh8D5x6h/jPgAzNpSpJ0dH5zUpI6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSerMpMGd5KYke5NsGaidnuSuJE+059NaPUm+kGRbkoeTvHsum5ekUTTMFfeXgAsPq10D3F1Vq4C72z7Ah4FV7bEeuGF22pQkHTJpcFfVd4FnDitfDNzctm8GLhmof7km3AucmmTpbDUrSZr+GveSqtrdtvcAS9r2MmDHwLidrfYaSdYn2ZRk0759+6bZhiSNnhn/5WRVFVDTOG9DVY1X1fjY2NhM25CkkTHd4H7q0BJIe97b6ruAFQPjlreaJGmWTDe4NwLr2vY64LaB+kfb3SVrgecHllQkSbPg+MkGJPkacD5wZpKdwB8AfwjcmuRK4KfApW34HcBFwDbg58DH5qBnSRppkwZ3VV1+lEMfOMLYAq6eaVOSpKPzm5OS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjozaXAnuSnJ3iRbBmqfSbIryeb2uGjg2LVJtiV5LMmH5qpxSRpVw1xxfwm48Aj166tqdXvcAZDkHOAy4NfaOf8zyaLZalaSNERwV9V3gWeGfL2LgVuqan9V/YSJX3tfM4P+JEmHmcka98eTPNyWUk5rtWXAjoExO1vtNZKsT7IpyaZ9+/bNoA1JGi3TDe4bgF8FVgO7gT+e6gtU1YaqGq+q8bGxsWm2IUmjZ1rBXVVPVdXBqnoF+FN+uRyyC1gxMHR5q0mSZsm0gjvJ0oHd3wYO3XGyEbgsyYlJzgZWAffPrEVJ0qDjJxuQ5GvA+cCZSXYCfwCcn2Q1UMB24HcBqmprkluBR4ADwNVVdXBuWpek0TRpcFfV5Uco3/g6468DrptJU5Kko/Obk5LUGYNbkjpjcEtSZwxuSeqMwS1JnZn0rhJplOx/4Wn2v/g0x5/0Zk45Y/l8tyMdkcGtkbfjb/+SXzz7/wDY/8I+9r+wj7e+41ze+aH/MM+dSUdmcGvk/f3en/D3Tz05321IQ3ONW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnJg3uJCuS3JPkkSRbk3yi1U9PcleSJ9rzaa2eJF9Isi3Jw0nePdeTkKRRMswV9wHgU1V1DrAWuDrJOcA1wN1VtQq4u+0DfJiJX3dfBawHbpj1riVphE0a3FW1u6q+37ZfBB4FlgEXAze3YTcDl7Tti4Ev14R7gVOTLJ31ziVpRE1pjTvJSuA84D5gSVXtbof2AEva9jJgx8BpO1vt8Ndan2RTkk379u2bYtuSNLqGDu4kbwa+AXyyql4YPFZVBdRU3riqNlTVeFWNj42NTeVUSRppQwV3ksVMhPZXq+qbrfzUoSWQ9ry31XcBKwZOX95qkqRZMMxdJQFuBB6tqs8PHNoIrGvb64DbBuofbXeXrAWeH1hSkSTN0DC/gPM+4Argh0k2t9rvA38I3JrkSuCnwKXt2B3ARcA24OfAx2a1Y0kacZMGd1V9D8hRDn/gCOMLuHqGfUmSjsJvTkpSZwxuSeqMwS1JnTG4NfJOeNNpr6kd/Md/4ODL++ehG2lyBrdG3tt+/YLX1F7a/Tj/8LOd89CNNDmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTPD/FjwiiT3JHkkydYkn2j1zyTZlWRze1w0cM61SbYleSzJh+ZyApI0aob5seADwKeq6vtJ3gI8mOSuduz6qvqvg4OTnANcBvwa8M+Bv0nyL6rq4Gw2LkmjatIr7qraXVXfb9svAo8Cy17nlIuBW6pqf1X9hIlfe18zG81Kkqa4xp1kJXAecF8rfTzJw0luSnLoZ0SWATsGTtvJ6we9JGkKhg7uJG8GvgF8sqpeAG4AfhVYDewG/ngqb5xkfZJNSTbt27dvKqdK0kgbKriTLGYitL9aVd8EqKqnqupgVb0C/Cm/XA7ZBawYOH15q/0TVbWhqsaranxsbGwmc5CkkTLMXSUBbgQerarPD9SXDgz7bWBL294IXJbkxCRnA6uA+2evZUkabcPcVfI+4Argh0k2t9rvA5cnWQ0UsB34XYCq2prkVuARJu5Iudo7SiRp9kwa3FX1PSBHOHTH65xzHXDdDPqSJB2F35yUpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqzDD/rKvUpS9+8Yvceeedk45bfupi1q09jRz2b2Bee+217Hzu5aHea+3atXz605+eTpvSlBncWrAeeughvv3tb0867px3jPE7479FFp3EKzXxn8RxOcim+/+Wex/ZOdR7HXecf3jVG8fg1sj70d89zf/d8gynrLiSFw6cAcCbFj3Ppf9mH/c+8vV57k56LS8TNPJeqWLr82t49uUlHKzFHKzFvHDgTA7kn813a9IRGdwScKAWc/gPPe3bv+LIg6V5NsyPBZ+U5P4kDyXZmuSzrX52kvuSbEvy9SQntPqJbX9bO75ybqcgzdzJi15i4udTf+ltJ+6Yn2akSQxzxb0fuKCqzgVWAxcmWQv8EXB9Vb0TeBa4so2/Eni21a9v46Rj2r98ywMsP/kJTjnuWZ595qf84rkf8OILw/3FpPRGG+bHggt4qe0ubo8CLgB+p9VvBj4D3ABc3LYB/gr4H0nSXkc6Jn37uw+xdOuPOXCwuGvTk+x/+QCHX4FLx4qh7ipJsgh4EHgn8CfAk8BzVXWgDdkJLGvby4AdAFV1IMnzwBnA00d7/T179vC5z31uWhOQjmbz5s1Djx32tr+jefzxx/0Ma1bt2bPnqMeGCu6qOgisTnIq8C3gXTNtKsl6YD3AsmXLuOKKK2b6ktI/sWXLFu6999435L3OOussP8OaVV/5yleOemxK93FX1XNJ7gHeC5ya5Ph21b0c2NWG7QJWADuTHA+8FfjZEV5rA7ABYHx8vN7+9rdPpRVpUqeccsob9l4nnXQSfoY1mxYvXnzUY8PcVTLWrrRJcjLwQeBR4B7gI23YOuC2tr2x7dOOf8f1bUmaPcNccS8Fbm7r3McBt1bV7UkeAW5J8l+AHwA3tvE3An+eZBvwDHDZHPQtSSNrmLtKHgbOO0L9x8CaI9R/Afy7WelOkvQafnNSkjpjcEtSZ/zXAbVgnXvuuVxyySVvyHutWfOaVUNpzhjcWrCuuuoqrrrqqvluQ5p1LpVIUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4M82PBJyW5P8lDSbYm+WyrfynJT5Jsbo/VrZ4kX0iyLcnDSd4915OQpFEyzL/HvR+4oKpeSrIY+F6S/92O/V5V/dVh4z8MrGqP9wA3tGdJ0iyY9Iq7JrzUdhe3R73OKRcDX27n3QucmmTpzFuVJMGQa9xJFiXZDOwF7qqq+9qh69pyyPVJTmy1ZcCOgdN3tpokaRYMFdxVdbCqVgPLgTVJfh24FngX8K+B04FPT+WNk6xPsinJpn379k2xbUkaXVO6q6SqngPuAS6sqt1tOWQ/8GfAoV9L3QWsGDhteasd/lobqmq8qsbHxsam170kjaBh7ioZS3Jq2z4Z+CDwo0Pr1kkCXAJsaadsBD7a7i5ZCzxfVbvnpHtJGkHD3FWyFLg5ySImgv7Wqro9yXeSjAEBNgOHfk77DuAiYBvwc+Bjs9+2JI2uSYO7qh4GzjtC/YKjjC/g6pm3Jkk6Er85KUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOpOqmu8eSPIi8Nh89zFHzgSenu8m5sBCnRcs3Lk5r768o6rGjnTg+De6k6N4rKrG57uJuZBk00Kc20KdFyzcuTmvhcOlEknqjMEtSZ05VoJ7w3w3MIcW6twW6rxg4c7NeS0Qx8RfTkqShnesXHFLkoY078Gd5MIkjyXZluSa+e5nqpLclGRvki0DtdOT3JXkifZ8WqsnyRfaXB9O8u756/z1JVmR5J4kjyTZmuQTrd713JKclOT+JA+1eX221c9Ocl/r/+tJTmj1E9v+tnZ85Xz2P5kki5L8IMntbX+hzGt7kh8m2ZxkU6t1/VmciXkN7iSLgD8BPgycA1ye5Jz57GkavgRceFjtGuDuqloF3N32YWKeq9pjPXDDG9TjdBwAPlVV5wBrgavb/za9z20/cEFVnQusBi5Mshb4I+D6qnon8CxwZRt/JfBsq1/fxh3LPgE8OrC/UOYF8BtVtXrg1r/eP4vTV1Xz9gDeC9w5sH8tcO189jTNeawEtgzsPwYsbdtLmbhPHeB/AZcfadyx/gBuAz64kOYGnAJ8H3gPE1/gOL7VX/1cAncC723bx7dxme/ejzKf5UwE2AXA7UAWwrxaj9uBMw+rLZjP4lQf871UsgzYMbC/s9V6t6SqdrftPcCStt3lfNsfo88D7mMBzK0tJ2wG9gJ3AU8Cz1XVgTZksPdX59WOPw+c8cZ2PLT/BvxH4JW2fwYLY14ABfx1kgeTrG+17j+L03WsfHNywaqqStLtrTtJ3gx8A/hkVb2Q5NVjvc6tqg4Cq5OcCnwLeNc8tzRjSf4tsLeqHkxy/nz3MwfeX1W7krwNuCvJjwYP9vpZnK75vuLeBawY2F/ear17KslSgPa8t9W7mm+SxUyE9ler6putvCDmBlBVzwH3MLGEcGqSQxcyg72/Oq92/K3Az97gVofxPuC3kmwHbmFiueS/0/+8AKiqXe15LxP/Z7uGBfRZnKr5Du4HgFXtb75PAC4DNs5zT7NhI7Cuba9jYn34UP2j7W+91wLPD/xR75iSiUvrG4FHq+rzA4e6nluSsXalTZKTmVi3f5SJAP9IG3b4vA7N9yPAd6otnB5LquraqlpeVSuZ+O/oO1X17+l8XgBJ3pTkLYe2gd8EttD5Z3FG5nuRHbgIeJyJdcb/NN/9TKP/rwG7gZeZWEu7kom1wruBJ4C/AU5vY8PEXTRPAj8Exue7/9eZ1/uZWFd8GNjcHhf1PjfgXwE/aPPaAvznVv8V4H5gG/CXwImtflLb39aO/8p8z2GIOZ4P3L5Q5tXm8FB7bD2UE71/Fmfy8JuTktSZ+V4qkSRNkcEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1Jn/j9GmGPcW9gN+AAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"cQbfxnmTBzKh","colab_type":"text"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"jgJDkoBmBzKi","colab_type":"text"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."]},{"cell_type":"code","metadata":{"id":"WD_LjvwEBzKi","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YqhVV74oBzKl","colab_type":"code","colab":{}},"source":["# Build a simple neural network that predicts policy logits. \n","# Keep it simple: CartPole isn't worth deep architectures.\n","model = nn.Sequential(\n","  nn.Linear(state_dim[0], 128),\n","  nn.ReLU(),\n","  nn.Linear(128, 64),\n","  nn.ReLU(),\n","  nn.Linear(64, n_actions),\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rCULUfbVBzKo","colab_type":"text"},"source":["#### Predict function"]},{"cell_type":"code","metadata":{"id":"1DtaLEX1F-Ds","colab_type":"code","colab":{}},"source":["from torch.nn import functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ok5YogIYBzKp","colab_type":"code","colab":{}},"source":["def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    # convert states, compute logits, use softmax to get probability\n","    states = torch.FloatTensor(states)\n","    logits = model(states).detach()\n","    probs = F.softmax(logits, dim=-1).numpy()\n","    return probs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n6pl7PgMBzKr","colab_type":"code","colab":{}},"source":["test_states = np.array([env.reset() for _ in range(5)])\n","test_probas = predict_probs(test_states)\n","assert isinstance(\n","    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n","assert tuple(test_probas.shape) == (\n","    test_states.shape[0], env.action_space.n), \"wrong output shape: %s\" % np.shape(test_probas)\n","assert np.allclose(np.sum(test_probas, axis=1),\n","                   1), \"probabilities do not sum to 1\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2_UGfWUOBzKu","colab_type":"text"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","metadata":{"id":"lUUYQ672BzKu","colab_type":"code","colab":{}},"source":["def generate_session(t_max=1000):\n","    \"\"\" \n","    play a full session with REINFORCE agent and train at the session end.\n","    returns sequences of states, actions andrewards\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(np.array([s]))[0]\n","\n","        # Sample action with given probabilities.\n","        a = np.random.choice(np.arange(n_actions), p=action_probs)\n","        new_s, r, done, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Shey2691BzKx","colab_type":"code","colab":{}},"source":["# test it\n","states, actions, rewards = generate_session()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgtHx1E5HppP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"53ded1af-ba1d-425a-97f5-689c5bf9b8a2","executionInfo":{"status":"ok","timestamp":1591572608274,"user_tz":-180,"elapsed":2048,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["len(states)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"fnE5e9HlBzK0","colab_type":"text"},"source":["### Computing cumulative rewards"]},{"cell_type":"code","metadata":{"id":"o3F5p6KcBzK0","colab_type":"code","colab":{}},"source":["def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    take a list of immediate rewards r(s,a) for the whole session \n","    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    The simple way to compute cumulative rewards is to iterate from last to first time tick\n","    and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    #<your code here >\n","\n","    G = np.zeros(len(rewards))\n","    G[-1] = rewards[-1]\n","    for idx in range(-2, -len(rewards)-1, -1):\n","        G[idx] = rewards[idx]+gamma * G[idx+1]\n","\n","    return G #< array of cumulative rewards >"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgWCFjfzBzK2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a0e241e9-9f04-4146-c674-5643f9996d7e","executionInfo":{"status":"ok","timestamp":1591573058819,"user_tz":-180,"elapsed":819,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["get_cumulative_rewards(rewards)\n","assert len(get_cumulative_rewards(list(range(100)))) == 100\n","assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n","                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(get_cumulative_rewards(\n","    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(get_cumulative_rewards(\n","    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":27,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"82amc4TOBzK5","colab_type":"text"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n","\n","\n","Following the REINFORCE algorithm, we can define our objective as follows: \n","\n","$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n","\n","When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"]},{"cell_type":"code","metadata":{"id":"ywNfaApjBzK5","colab_type":"code","colab":{}},"source":["def to_one_hot(y_tensor, ndims):\n","    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n","    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n","    y_one_hot = torch.zeros(\n","        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n","    return y_one_hot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nMCGVkSUPzrA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"d6747786-e823-4223-98aa-98e1df265712","executionInfo":{"status":"ok","timestamp":1591574775597,"user_tz":-180,"elapsed":1548,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["to_one_hot(torch.tensor([1, 2, 3]), ndims=4)"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 1., 0., 0.],\n","        [0., 0., 1., 0.],\n","        [0., 0., 0., 1.]])"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"K9k14HsuBzK9","colab_type":"code","colab":{}},"source":["# Your code: define optimizers\n","optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n","\n","\n","def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-3):\n","    \"\"\"\n","    Takes a sequence of states, actions and rewards produced by generate_session.\n","    Updates agent's weights by following the policy gradient above.\n","    Please use Adam optimizer with default parameters.\n","    \"\"\"\n","\n","    # cast everything into torch tensors\n","    states = torch.tensor(states, dtype=torch.float32)\n","    actions = torch.tensor(actions, dtype=torch.int32)\n","    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n","    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n","\n","    # predict logits, probas and log-probas using an agent.\n","    logits = model(states)\n","    probs = nn.functional.softmax(logits, -1)\n","    log_probs = nn.functional.log_softmax(logits, -1)\n","\n","    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n","        \"please use compute using torch tensors and don't use predict_probs function\"\n","\n","    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n","    log_probs_for_actions = torch.sum(\n","        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n","   \n","    # Compute loss here. Don't forget entropy regularization with `entropy_coef` \n","    entropy = - torch.mean(torch.sum(probs*log_probs), dim=-1)#< your code >\n","    loss = - torch.mean(log_probs_for_actions * cumulative_returns) - entropy * entropy_coef #< your code\n","\n","    # Gradient descent step\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    # technical: return session rewards to print them later\n","    return np.sum(rewards)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bota7yQ3BzLB","colab_type":"text"},"source":["### The actual training"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"8u8exaFbBzLB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"d3615494-5a4e-410d-a5ba-f8b13d2f5e63","executionInfo":{"status":"ok","timestamp":1591574580110,"user_tz":-180,"elapsed":88833,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session())\n","               for _ in range(100)]  # generate new sessions\n","    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n","    if np.mean(rewards) > 500:\n","        print(\"You Win!\")  # but you can train even further\n","        break"],"execution_count":32,"outputs":[{"output_type":"stream","text":["mean reward:254.150\n","mean reward:147.620\n","mean reward:114.590\n","mean reward:224.410\n","mean reward:157.750\n","mean reward:118.940\n","mean reward:111.540\n","mean reward:91.110\n","mean reward:129.190\n","mean reward:185.340\n","mean reward:170.480\n","mean reward:193.700\n","mean reward:441.080\n","mean reward:633.950\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tSwShNgGBzLE","colab_type":"text"},"source":["### Video"]},{"cell_type":"code","metadata":{"id":"hT4RPmInBzLF","colab_type":"code","colab":{}},"source":["# record sessions\n","import gym.wrappers\n","env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n","                           directory=\"videos\", force=True)\n","sessions = [generate_session() for _ in range(100)]\n","env.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"al_KHVHKBzLH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":501},"outputId":"4c0b30f5-dc4b-4d7c-e87c-0ab172c6a1f2","executionInfo":{"status":"ok","timestamp":1591574844101,"user_tz":-180,"elapsed":18642,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# show video\n","from IPython.display import HTML\n","import os\n","\n","video_names = list(\n","    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be the _last_ video. Try other indices"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"./videos/openaigym.video.0.124.video000000.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"Ja7k16AjBzLJ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jqp6FxrKBzLM","colab_type":"text"},"source":["### Bonus area: solving Acrobot-v1\n","Try to solve more complex environment using Policy gradient method.\n","*Hint: you will need add some imporovements to the original REINFORCE (e.g. Advantage Actor Critic or anything else).*"]},{"cell_type":"code","metadata":{"id":"z1UFgwWwBzLM","colab_type":"code","colab":{}},"source":["env = gym.make(\"Acrobot-v1\")\n","env.reset()\n","\n","plt.imshow(env.render(\"rgb_array\"))\n","state_dim = env.reset().shape[0]\n","n_actions = env.action_space.n\n","\n","print(state_dim, n_actions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwqX_qGyBzLO","colab_type":"code","colab":{}},"source":["<Your beautiful code here>"],"execution_count":0,"outputs":[]}]}